"""
ë§¨íì˜ ê²½ì œí•™ - RAG íŒŒì´í”„ë¼ì¸
===============================
PDF ì¶”ì¶œ â†’ í…ìŠ¤íŠ¸ ì²­í‚¹ â†’ Gemini ì„ë² ë”© â†’ ChromaDB ì €ì¥

ì‚¬ìš©ë²•:
  python rag/pipeline.py                          # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
  python rag/pipeline.py --step extract           # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œë§Œ
  python rag/pipeline.py --step chunk             # í…ìŠ¤íŠ¸ ì²­í‚¹ë§Œ
  python rag/pipeline.py --step embed             # ì„ë² ë”© + DB ì €ì¥ë§Œ
  python rag/pipeline.py --api-key YOUR_KEY       # API í‚¤ ì§ì ‘ ì§€ì •
"""

import os
import sys
import json
import re
import time
import argparse
import hashlib
from pathlib import Path

# Windowsì—ì„œ UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    sys.stderr.reconfigure(encoding='utf-8', errors='replace')

# â”€â”€ ê²½ë¡œ ì„¤ì • â”€â”€
BASE_DIR = Path(__file__).resolve().parent.parent
RAW_DB_DIR = BASE_DIR / ".raw_db"
RAG_DIR = BASE_DIR / "rag"
DATA_DIR = RAG_DIR / "data"
CHROMA_DIR = RAG_DIR / "chroma_db"

# â”€â”€ ì²­í‚¹ ì„¤ì • â”€â”€
CHUNK_SIZE = 800       # ì²­í¬ í¬ê¸° (ë¬¸ì)
CHUNK_OVERLAP = 150    # ì˜¤ë²„ë© (ë¬¸ì)
MIN_CHUNK_SIZE = 100   # ìµœì†Œ ì²­í¬ í¬ê¸°

# â”€â”€ ì„ë² ë”© ì„¤ì • â”€â”€
EMBEDDING_MODEL = "models/gemini-embedding-001"
EMBEDDING_BATCH_SIZE = 50   # Gemini API ë°°ì¹˜ í¬ê¸°
EMBEDDING_RATE_LIMIT = 0.5  # API í˜¸ì¶œ ê°„ ëŒ€ê¸°ì‹œê°„ (ì´ˆ)


def step1_extract_pdfs():
    """Step 1: PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    print("\n" + "=" * 60)
    print("ğŸ“„ Step 1: PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ")
    print("=" * 60)

    DATA_DIR.mkdir(parents=True, exist_ok=True)

    pdf_files = sorted(RAW_DB_DIR.glob("*.pdf"))
    if not pdf_files:
        print("âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:", RAW_DB_DIR)
        return False

    print(f"ğŸ“ ë°œê²¬ëœ PDF: {len(pdf_files)}ê°œ")

    # pdfplumberë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    try:
        import pdfplumber
    except ImportError:
        print("âŒ pdfplumber ì„¤ì¹˜ í•„ìš”: pip install pdfplumber")
        return False

    all_pages = []
    total_chars = 0

    for pdf_path in pdf_files:
        print(f"\nğŸ“– ì²˜ë¦¬ ì¤‘: {pdf_path.name}")
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_idx, page in enumerate(pdf.pages):
                    text = page.extract_text()
                    if not text or len(text.strip()) < 20:
                        continue

                    # í…ìŠ¤íŠ¸ ì •ì œ
                    text = clean_text(text)
                    
                    # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì • (íŒŒì¼ëª…ì—ì„œ)
                    # ì˜ˆ: 001-100.pdf â†’ 1~100
                    page_range = pdf_path.stem.split("-")
                    if len(page_range) == 2:
                        start_page = int(page_range[0])
                        estimated_page = start_page + page_idx
                    else:
                        estimated_page = page_idx + 1

                    page_data = {
                        "source_file": pdf_path.name,
                        "page_index": page_idx,
                        "estimated_page": estimated_page,
                        "text": text,
                        "char_count": len(text)
                    }
                    all_pages.append(page_data)
                    total_chars += len(text)

                print(f"   âœ… {len(pdf.pages)}í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ")
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {e}")
            continue

    # ê²°ê³¼ ì €ì¥
    output_path = DATA_DIR / "extracted_pages.jsonl"
    with open(output_path, "w", encoding="utf-8") as f:
        for page in all_pages:
            f.write(json.dumps(page, ensure_ascii=False) + "\n")

    print(f"\n{'â”€' * 40}")
    print(f"ğŸ“Š ì¶”ì¶œ ê²°ê³¼:")
    print(f"   ì´ í˜ì´ì§€: {len(all_pages)}ê°œ")
    print(f"   ì´ ë¬¸ì ìˆ˜: {total_chars:,}ì")
    print(f"   ì €ì¥ ìœ„ì¹˜: {output_path}")
    return True


def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ì œ"""
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
    text = re.sub(r'\n{3,}', '\n\n', text)
    # í˜ì´ì§€ í—¤ë”/í‘¸í„° íŒ¨í„´ ì œê±° (ìˆ«ìë§Œ ìˆëŠ” ì¤„)
    text = re.sub(r'^\d+\s*$', '', text, flags=re.MULTILINE)
    # ì—°ì† ê³µë°± ì •ë¦¬
    text = re.sub(r' {3,}', '  ', text)
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    return text


def step2_chunk_text():
    """Step 2: í…ìŠ¤íŠ¸ ì²­í‚¹"""
    print("\n" + "=" * 60)
    print("âœ‚ï¸  Step 2: í…ìŠ¤íŠ¸ ì²­í‚¹")
    print("=" * 60)

    pages_path = DATA_DIR / "extracted_pages.jsonl"
    if not pages_path.exists():
        print("âŒ ì¶”ì¶œëœ í˜ì´ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Step 1ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return False

    # í˜ì´ì§€ ë¡œë“œ
    pages = []
    with open(pages_path, "r", encoding="utf-8") as f:
        for line in f:
            pages.append(json.loads(line.strip()))

    print(f"ğŸ“„ ë¡œë“œëœ í˜ì´ì§€: {len(pages)}ê°œ")

    # ì±•í„° ê°ì§€ë¥¼ ìœ„í•œ íŒ¨í„´
    chapter_patterns = [
        re.compile(r'(?:ì œ?\s*)?(\d{1,2})\s*[ì¥í¸]\s*[.:]?\s*(.+)', re.MULTILINE),
        re.compile(r'CHAPTER\s*(\d{1,2})\s*[.:]?\s*(.+)', re.IGNORECASE | re.MULTILINE),
        re.compile(r'(?:Part|íŒŒíŠ¸)\s*(\d{1,2})\s*[.:]?\s*(.+)', re.IGNORECASE | re.MULTILINE),
    ]

    chunks = []
    current_chapter = "Unknown"
    current_part = "Unknown"
    chunk_id = 0

    for page in pages:
        text = page["text"]
        
        # ì±•í„°/íŒŒíŠ¸ ê°ì§€
        for pattern in chapter_patterns:
            match = pattern.search(text[:200])  # í˜ì´ì§€ ìƒë‹¨ì—ì„œë§Œ ê²€ìƒ‰
            if match:
                num = match.group(1)
                title = match.group(2).strip()
                if 'ì¥' in pattern.pattern or 'CHAPTER' in pattern.pattern.upper():
                    current_chapter = f"Chapter {num}: {title}"
                else:
                    current_part = f"Part {num}: {title}"
                break

        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        page_chunks = create_chunks(
            text,
            chunk_size=CHUNK_SIZE,
            overlap=CHUNK_OVERLAP,
            min_size=MIN_CHUNK_SIZE
        )

        for chunk_text in page_chunks:
            chunk_id += 1
            chunk_data = {
                "id": f"chunk_{chunk_id:05d}",
                "text": chunk_text,
                "metadata": {
                    "source_file": page["source_file"],
                    "estimated_page": page["estimated_page"],
                    "chapter": current_chapter,
                    "part": current_part,
                    "char_count": len(chunk_text),
                    "chunk_index": chunk_id
                }
            }
            chunks.append(chunk_data)

    # ê²°ê³¼ ì €ì¥
    output_path = DATA_DIR / "chunks.jsonl"
    with open(output_path, "w", encoding="utf-8") as f:
        for chunk in chunks:
            f.write(json.dumps(chunk, ensure_ascii=False) + "\n")

    # í†µê³„
    avg_size = sum(c["metadata"]["char_count"] for c in chunks) / len(chunks) if chunks else 0
    
    print(f"\n{'â”€' * 40}")
    print(f"ğŸ“Š ì²­í‚¹ ê²°ê³¼:")
    print(f"   ì´ ì²­í¬ ìˆ˜: {len(chunks):,}ê°œ")
    print(f"   í‰ê·  í¬ê¸°: {avg_size:.0f}ì")
    print(f"   ì²­í¬ í¬ê¸°: {CHUNK_SIZE}ì / ì˜¤ë²„ë©: {CHUNK_OVERLAP}ì")
    print(f"   ì €ì¥ ìœ„ì¹˜: {output_path}")
    return True


def create_chunks(text, chunk_size=800, overlap=150, min_size=100):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ë¬¸ì¥ ê²½ê³„ ê³ ë ¤)"""
    if len(text) <= chunk_size:
        return [text] if len(text) >= min_size else []

    chunks = []
    
    # ë¬¸ì¥ ë¶„ë¦¬ (í•œêµ­ì–´ + ì˜ì–´)
    sentences = re.split(r'(?<=[.!?ã€‚]\s)|(?<=\n\n)', text)
    
    current_chunk = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        if len(current_chunk) + len(sentence) <= chunk_size:
            current_chunk += " " + sentence if current_chunk else sentence
        else:
            if len(current_chunk) >= min_size:
                chunks.append(current_chunk.strip())
            
            # ì˜¤ë²„ë© ì²˜ë¦¬: ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ë‹¤ìŒ ì²­í¬ ì‹œì‘ì— í¬í•¨
            if overlap > 0 and current_chunk:
                overlap_text = current_chunk[-overlap:]
                current_chunk = overlap_text + " " + sentence
            else:
                current_chunk = sentence

    # ë§ˆì§€ë§‰ ì²­í¬
    if current_chunk and len(current_chunk) >= min_size:
        chunks.append(current_chunk.strip())

    return chunks


def step3_build_vectordb(api_key=None):
    """Step 3: ì„ë² ë”© ìƒì„± + ChromaDB ì €ì¥ (ì¬ì‹œì‘ ê°€ëŠ¥)"""
    print("\n" + "=" * 60)
    print("Step 3: ì„ë² ë”© ìƒì„± + ChromaDB ì €ì¥")
    print("=" * 60)

    # API í‚¤ í™•ì¸ (.env ìš°ì„  â†’ í™˜ê²½ë³€ìˆ˜ â†’ ì¸ì)
    if not api_key:
        # 1ìˆœìœ„: .env íŒŒì¼
        env_path = BASE_DIR / ".env"
        if env_path.exists():
            with open(env_path, "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip().startswith("GEMINI_API_KEY=") or line.strip().startswith("GOOGLE_API_KEY="):
                        api_key = line.strip().split("=", 1)[1].strip().strip('"').strip("'")
                        print(f"   .envì—ì„œ API í‚¤ ë¡œë“œë¨")
                        break
    
    if not api_key:
        # 2ìˆœìœ„: í™˜ê²½ë³€ìˆ˜
        api_key = os.environ.get("GEMINI_API_KEY") or os.environ.get("GOOGLE_API_KEY")

    if not api_key:
        print("[ERROR] Gemini API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("   ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë°©ë²•ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”:")
        print("   1) python rag/pipeline.py --api-key YOUR_KEY")
        print("   2) í™˜ê²½ë³€ìˆ˜: set GEMINI_API_KEY=YOUR_KEY")
        print("   3) .env íŒŒì¼: GEMINI_API_KEY=YOUR_KEY")
        return False

    # ì²­í¬ ë¡œë“œ
    chunks_path = DATA_DIR / "chunks.jsonl"
    if not chunks_path.exists():
        print("[ERROR] ì²­í¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Step 2ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return False

    chunks = []
    with open(chunks_path, "r", encoding="utf-8") as f:
        for line in f:
            chunks.append(json.loads(line.strip()))

    print(f"   ë¡œë“œëœ ì²­í¬: {len(chunks):,}ê°œ")

    # Gemini API ì—°ê²° í™•ì¸ (ì§ì ‘ HTTP í˜¸ì¶œ ì‚¬ìš© â€” deprecated ë¼ì´ë¸ŒëŸ¬ë¦¬ ìš°íšŒ)
    import urllib.request
    import urllib.error

    def embed_texts(texts_list, api_key_val):
        """Gemini REST APIë¡œ ì§ì ‘ ì„ë² ë”© ìƒì„± (ë°°ì¹˜)"""
        model_name = EMBEDDING_MODEL.replace("models/", "")
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:batchEmbedContents?key={api_key_val}"
        
        requests_body = []
        for text in texts_list:
            requests_body.append({
                "model": EMBEDDING_MODEL,
                "content": {"parts": [{"text": text}]},
                "taskType": "RETRIEVAL_DOCUMENT"
            })
        
        payload = json.dumps({"requests": requests_body}).encode("utf-8")
        req = urllib.request.Request(url, data=payload, method="POST")
        req.add_header("Content-Type", "application/json")
        
        try:
            with urllib.request.urlopen(req, timeout=60) as resp:
                data = json.loads(resp.read().decode())
            return [item["values"] for item in data["embeddings"]]
        except urllib.error.HTTPError as e:
            body = e.read().decode()
            raise Exception(f"HTTP {e.code}: {body[:300]}")

    def embed_single(text, api_key_val):
        """Gemini REST APIë¡œ ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        model_name = EMBEDDING_MODEL.replace("models/", "")
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:embedContent?key={api_key_val}"
        
        payload = json.dumps({
            "model": EMBEDDING_MODEL,
            "content": {"parts": [{"text": text}]},
            "taskType": "RETRIEVAL_DOCUMENT"
        }).encode("utf-8")
        req = urllib.request.Request(url, data=payload, method="POST")
        req.add_header("Content-Type", "application/json")
        
        try:
            with urllib.request.urlopen(req, timeout=30) as resp:
                data = json.loads(resp.read().decode())
            return data["embedding"]["values"]
        except urllib.error.HTTPError as e:
            body = e.read().decode()
            raise Exception(f"HTTP {e.code}: {body[:300]}")

    # ì—°ê²° í…ŒìŠ¤íŠ¸
    print(f"   [DEBUG] API key: {repr(api_key[:8])}...{repr(api_key[-4:])}, len={len(api_key)}")
    try:
        test_emb = embed_single("test", api_key)
        print(f"   Gemini Embedding API ì—°ê²° ì„±ê³µ (ì°¨ì›: {len(test_emb)}, ëª¨ë¸: {EMBEDDING_MODEL})")
    except Exception as e:
        print(f"[ERROR] Gemini Embedding API ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

    # ChromaDB ì„¤ì •
    try:
        import chromadb
        from chromadb.config import Settings
    except ImportError:
        print("[ERROR] chromadb ì„¤ì¹˜ í•„ìš”: pip install chromadb")
        return False

    # ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    CHROMA_DIR.mkdir(parents=True, exist_ok=True)
    client = chromadb.PersistentClient(path=str(CHROMA_DIR))
    
    # ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° (ìˆìœ¼ë©´ ì¬ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒì„±) â€” ì¬ì‹œì‘ ì§€ì›
    collection_name = "mankiw_economics"
    try:
        collection = client.get_collection(collection_name)
        existing_count = collection.count()
        print(f"   ê¸°ì¡´ ì»¬ë ‰ì…˜ ë°œê²¬: {existing_count:,}ê°œ ë¬¸ì„œ")
    except Exception:
        collection = client.create_collection(
            name=collection_name,
            metadata={"description": "ë§¨íì˜ ê²½ì œí•™ ì œ9íŒ êµê³¼ì„œ ë²¡í„° DB"}
        )
        existing_count = 0
        print(f"   ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±ë¨: {collection_name}")

    # ì´ë¯¸ ì„ë² ë”©ëœ ì²­í¬ ID í™•ì¸ (ì¬ì‹œì‘ ì§€ì›)
    existing_ids = set()
    if existing_count > 0:
        try:
            stored = collection.get(include=[])
            existing_ids = set(stored['ids'])
            print(f"   ì´ë¯¸ ì„ë² ë”©ëœ ì²­í¬: {len(existing_ids):,}ê°œ (ê±´ë„ˆëœë‹ˆë‹¤)")
        except Exception as e:
            print(f"   ê¸°ì¡´ ID ì¡°íšŒ ì‹¤íŒ¨: {e}")

    # ì„ë² ë”©í•  ì²­í¬ í•„í„°ë§
    remaining_chunks = [c for c in chunks if c['id'] not in existing_ids]
    
    if not remaining_chunks:
        print("\n   ëª¨ë“  ì²­í¬ê°€ ì´ë¯¸ ì„ë² ë”©ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
        print(f"   DB í¬ê¸°: {collection.count():,}ê°œ ë¬¸ì„œ")
        return True

    print(f"   ì„ë² ë”©í•  ì²­í¬: {len(remaining_chunks):,}ê°œ (ì „ì²´ {len(chunks):,}ê°œ ì¤‘)")

    # ë°°ì¹˜ ì„ë² ë”© ìƒì„± + DB ì €ì¥
    batch_size = 20
    rate_limit = 1.2
    total_batches = (len(remaining_chunks) + batch_size - 1) // batch_size
    embedded_count = 0
    error_count = 0
    consecutive_errors = 0

    for batch_idx in range(0, len(remaining_chunks), batch_size):
        batch = remaining_chunks[batch_idx:batch_idx + batch_size]
        batch_num = batch_idx // batch_size + 1
        
        texts = [c["text"] for c in batch]
        ids = [c["id"] for c in batch]
        metadatas = [c["metadata"] for c in batch]

        try:
            # REST APIë¡œ ì„ë² ë”© ìƒì„±
            embeddings = embed_texts(texts, api_key)

            # ChromaDBì— ì €ì¥
            collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=texts,
                metadatas=metadatas
            )
            embedded_count += len(batch)
            consecutive_errors = 0
            
            # ì§„í–‰ë¥  í‘œì‹œ
            total_done = len(existing_ids) + embedded_count
            pct = (batch_num / total_batches) * 100
            bar = ">" * int(pct // 2.5) + "-" * (40 - int(pct // 2.5))
            print(f"\r   [{bar}] {pct:.1f}% ({total_done:,}/{len(chunks):,})", end="", flush=True)

            # Rate limit ëŒ€ê¸°
            if batch_idx + batch_size < len(remaining_chunks):
                time.sleep(rate_limit)

        except Exception as e:
            error_count += 1
            consecutive_errors += 1
            error_msg = str(e)
            print(f"\n   [WARN] ë°°ì¹˜ {batch_num} ì˜¤ë¥˜: {error_msg[:100]}")
            
            if consecutive_errors >= 5:
                print(f"\n   [ERROR] ì—°ì† {consecutive_errors}ë²ˆ ì˜¤ë¥˜ ë°œìƒ. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                print(f"   í˜„ì¬ê¹Œì§€ {len(existing_ids) + embedded_count:,}ê°œ ì„ë² ë”© ì™„ë£Œ (ì¬ì‹œì‘ ê°€ëŠ¥)")
                break
            
            wait_time = min(2 ** consecutive_errors, 60)
            print(f"   {wait_time}ì´ˆ ëŒ€ê¸° í›„ ê°œë³„ ì¬ì‹œë„...")
            time.sleep(wait_time)
            
            # ê°œë³„ ì²˜ë¦¬ë¡œ ì¬ì‹œë„
            for i, chunk in enumerate(batch):
                try:
                    emb = embed_single(chunk["text"], api_key)
                    collection.add(
                        ids=[chunk["id"]],
                        embeddings=[emb],
                        documents=[chunk["text"]],
                        metadatas=[chunk["metadata"]]
                    )
                    embedded_count += 1
                    consecutive_errors = 0
                    time.sleep(0.5)
                except Exception as e2:
                    print(f"\n   [FAIL] ì²­í¬ {chunk['id']}: {str(e2)[:80]}")

    final_count = collection.count()
    print(f"\n\n{'â”€' * 40}")
    print(f"   ì„ë² ë”© ê²°ê³¼:")
    print(f"   ì´ë²ˆ ì„¸ì…˜ ì„±ê³µ: {embedded_count:,}ê°œ")
    print(f"   ì´ì „ ì„¸ì…˜ í¬í•¨: {final_count:,}ê°œ")
    print(f"   ì˜¤ë¥˜ ë°°ì¹˜: {error_count}ê°œ")
    print(f"   DB ìœ„ì¹˜: {CHROMA_DIR}")
    print(f"   ì»¬ë ‰ì…˜: {collection_name}")
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    meta = {
        "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "total_chunks": final_count,
        "target_chunks": len(chunks),
        "embedding_model": EMBEDDING_MODEL,
        "chunk_size": CHUNK_SIZE,
        "chunk_overlap": CHUNK_OVERLAP,
        "source_files": [f.name for f in sorted(RAW_DB_DIR.glob("*.pdf"))],
        "complete": final_count >= len(chunks)
    }
    meta_path = CHROMA_DIR / "metadata.json"
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    if final_count < len(chunks):
        print(f"\n   [INFO] {len(chunks) - final_count:,}ê°œ ì²­í¬ê°€ ë‚¨ì•˜ìŠµë‹ˆë‹¤.")
        print(f"   ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤: python rag/pipeline.py --step embed")

    return True


def test_query(query="ìˆ˜ìš”ì™€ ê³µê¸‰ì˜ ê· í˜•", api_key=None):
    """ë²¡í„° DB í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬"""
    print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: \"{query}\"")
    print("â”€" * 40)

    if not api_key:
        api_key = os.environ.get("GEMINI_API_KEY") or os.environ.get("GOOGLE_API_KEY")

    if not api_key:
        env_path = BASE_DIR / ".env"
        if env_path.exists():
            with open(env_path, "r") as f:
                for line in f:
                    if line.strip().startswith("GEMINI_API_KEY=") or line.strip().startswith("GOOGLE_API_KEY="):
                        api_key = line.strip().split("=", 1)[1].strip().strip('"').strip("'")
                        break

    if not api_key:
        print("âŒ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    import google.generativeai as genai
    import chromadb

    genai.configure(api_key=api_key)
    client = chromadb.PersistentClient(path=str(CHROMA_DIR))
    collection = client.get_collection("mankiw_economics")

    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_result = genai.embed_content(
        model=EMBEDDING_MODEL,
        content=query,
        task_type="retrieval_query"
    )

    # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
    results = collection.query(
        query_embeddings=[query_result['embedding']],
        n_results=5
    )

    print(f"\nğŸ“‹ ìƒìœ„ 5ê°œ ê²°ê³¼:")
    for i, (doc, meta, dist) in enumerate(zip(
        results['documents'][0],
        results['metadatas'][0],
        results['distances'][0]
    )):
        print(f"\n  [{i+1}] (ìœ ì‚¬ë„: {1-dist:.4f})")
        print(f"      ğŸ“ {meta.get('source_file', 'N/A')} Â· p.{meta.get('estimated_page', 'N/A')}")
        print(f"      ğŸ“– {meta.get('chapter', 'N/A')}")
        print(f"      ğŸ“ {doc[:150]}...")


def main():
    parser = argparse.ArgumentParser(description="ë§¨íì˜ ê²½ì œí•™ RAG íŒŒì´í”„ë¼ì¸")
    parser.add_argument("--step", choices=["extract", "chunk", "embed", "test", "all"],
                       default="all", help="ì‹¤í–‰í•  ë‹¨ê³„")
    parser.add_argument("--api-key", type=str, help="Gemini API í‚¤")
    parser.add_argument("--query", type=str, default="ìˆ˜ìš”ì™€ ê³µê¸‰ì˜ ê· í˜•",
                       help="í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ (--step test ì‚¬ìš© ì‹œ)")
    args = parser.parse_args()

    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘  ë§¨íì˜ ê²½ì œí•™ RAG íŒŒì´í”„ë¼ì¸           â•‘")
    print("â•‘  PDF â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ ChromaDB       â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print(f"\nğŸ“ í”„ë¡œì íŠ¸: {BASE_DIR}")
    print(f"ğŸ“ PDF ì›ë³¸: {RAW_DB_DIR}")
    print(f"ğŸ“ ë°ì´í„°: {DATA_DIR}")
    print(f"ğŸ“ ë²¡í„°DB: {CHROMA_DIR}")

    start_time = time.time()
    success = True

    if args.step in ("extract", "all"):
        success = step1_extract_pdfs()
        if not success and args.step == "all":
            print("âŒ PDF ì¶”ì¶œ ì‹¤íŒ¨. íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return

    if args.step in ("chunk", "all"):
        success = step2_chunk_text()
        if not success and args.step == "all":
            print("âŒ ì²­í‚¹ ì‹¤íŒ¨. íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return

    if args.step in ("embed", "all"):
        success = step3_build_vectordb(api_key=args.api_key)

    if args.step == "test":
        test_query(query=args.query, api_key=args.api_key)

    elapsed = time.time() - start_time
    print(f"\nâ±ï¸  ì´ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!" if success else "âŒ íŒŒì´í”„ë¼ì¸ì— ì˜¤ë¥˜ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
